library("tm")
library("SnowballC") #text stemming
library("wordcloud")
library("RColorBrewer")
library("slam")
pacman::p_load(tm)
##########################
# Lire le fichier texte
filePath <- "microchip.xlsx"
text <- readLines(filePath)
# Charger les donn?es comme un corpus (ensemble de documents)
#La fonction VectorSource() se charge de la cr?ation du corpus de textes (ensemble de vecteurs de textes)
docs <- Corpus(VectorSource(text))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
#consultation du document
inspect(docs)
View(docs)
##########################
# Lire le fichier texte
filePath <- "microchip.csv"
text <- readLines(filePath)
# Charger les donnees comme un corpus (ensemble de documents)
#La fonction VectorSource() se charge de la creation du corpus de textes (ensemble de vecteurs de textes)
docs <- Corpus(VectorSource(text))
#consultation du document
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
#consultation du document
inspect(docs)
# Convertir le texte en minuscule
docs <- tm_map(docs, content_transformer(tolower))
# Supprimer les nombres
docs <- tm_map(docs, removeNumbers)
# Supprimer les ponctuations
docs <- tm_map(docs, removePunctuation)
# Supprimer les mots vides anglais
docs <- tm_map(docs, removeWords, c("la", "je","suis","ca","va","être","car","veux","à","mon","étant",
"une","on","doit","lui","plus","en","tout","des","de","d","les","du","qui","au","est","nous","parce","que",
"très","le","après","celle","notre","d","sur","ca","me","ses","qui","dans","a"))
# Supprimer les espaces vides suppl?mentaires
docs <- tm_map(docs, stripWhitespace)
# Text stemming
docs <- tm_map(docs, stemDocument)
#consultation du document
inspect(docs)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10) #on affiche que les 10 mots les plus fr?quencts
set.seed(123)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=123, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
require(xlsx)
worldCup <- read.xlsx(file.choose(), header = T, sheetIndex = 1)
worldCup
str(worldCup)
n.comment <- length(worldCup$ec)
comments <- worldCup$ec
n.comment <- length(worldCup$ef)
comments <- worldCup$ef
comments <- as.data.frame(comments)
View(comments)
#**************************
comments.txt <- sapply(comments, function(t)gettext())
# Ignore graphical Parameters to avoid input errors
comments.txt <- str_replace_all(comments.txt,"[^[:graph:]]", " ")
#**************************
users <- worldCup$ee
library("tidytext")
library("tm")
library("wordcloud")
library("networkD3")
library("rtweet")
library("plyr")
library("ggeasy")
library("plotly")
library("dplyr")
library("janeaustenr")
library("widyr")
# Ignore graphical Parameters to avoid input errors
comments.txt <- str_replace_all(comments.txt,"[^[:graph:]]", " ")
library(stringr)
# Ignore graphical Parameters to avoid input errors
comments.txt <- str_replace_all(comments.txt,"[^[:graph:]]", " ")
#**************************
users <- worldCup$ee
users <- as.data.frame(users)
#**************************
comments.txt <- sapply(comments, function(t)gettext())
View(comments.txt)
worldCup <- read.xlsx(file.choose(), header = T, sheetIndex = 1)
str(worldCup)
n.comment <- length(worldCup$ef)
comments <- worldCup$ef
comments <- as.data.frame(comments)
#**************************
comments.txt <- sapply(comments, function(t)gettext())
# Ignore graphical Parameters to avoid input errors
comments.txt <- str_replace_all(comments.txt,"[^[:graph:]]", " ")
comments.txt
#**************************
users <- worldCup$ee
users <- as.data.frame(users)
users_factor <- as.factor(worldCup$ee)
length(levels(users_factor))
corpus <- iconv(worldCup$ef, to = 'UTF-8')
corpus <- Corpus(VectorSource(corpus))
#********************
corpus <- clean.text(corpus)
library(twitteR)
library(ROAuth)
api_key <- "MNrOWdvkfmaxqyOmHM2koG9Hc"
api_secret <- "VdrQ83ysSiVWa4U76Rtko0PcUg97nEX9Sfx9pTD9j44oxBQEFg"
access_token <- "1472200254720299008-AgBRM4wMiXWBtL59vspWimKpKvZfuK"
access_token_secret <- "Lju6mxeCdc3nZssNHCNxgQuRP8Q35UO9kKcd6a6Yj3AeO"
#Note: This will ask us permission for direct authentication, type '1' for yes:
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
# extracting 4000 tweets related to global warming topic
tweets <- searchTwitter("#brexit", n=4000, lang="en")
# extracting 4000 tweets related to global warming topic
tweets <- searchTwitter("#covidvaccinepassport", n=1000, lang="en")
View(tweets)
# extracting 4000 tweets related to global warming topic
tweets <- searchTwitter("#covidvaccinepassport", n=350, lang="en")
# extracting 4000 tweets related to global warming topic
tweets <- searchTwitter("#VaccinePassports", n=500, lang="en")
n.tweet <- length(tweets)
# convert tweets to a data frame
tweets.df <- twListToDF(tweets)
View(tweets.df)
tweets.txt <- sapply(tweets, function(t)t$getText())
# Ignore graphical Parameters to avoid input errors
tweets.txt <- str_replace_all(tweets.txt,"[^[:graph:]]", " ")
## pre-processing text:
clean.text = function(x)
{
# convert to lower case
x = tolower(x)
# remove rt
x = gsub("rt", "", x)
# remove at
x = gsub("@\\w+", "", x)
# remove punctuation
x = gsub("[[:punct:]]", "", x)
# remove numbers
x = gsub("[[:digit:]]", "", x)
# remove links http
x = gsub("http\\w+", "", x)
# remove tabs
x = gsub("[ |\t]{2,}", "", x)
# remove blank spaces at the beginning
x = gsub("^ ", "", x)
# remove blank spaces at the end
x = gsub(" $", "", x)
# some other cleaning text
x = gsub('https://','',x)
x = gsub('http://','',x)
x = gsub('[^[:graph:]]', ' ',x)
x = gsub('[[:punct:]]', '', x)
x = gsub('[[:cntrl:]]', '', x)
x = gsub('\\d+', '', x)
x = str_replace_all(x,"[^[:graph:]]", " ")
return(x)
}
cleanText <- clean.text(tweets.txt)
# remove empty results (if any)
idx <- which(cleanText == " ")
cleanText <- cleanText[cleanText != " "]
tweets.df %<>%
mutate(
created = created %>%
# Remove zeros.
str_remove_all(pattern = '\\+0000') %>%
# Parse date.
parse_date_time(orders = '%y-%m-%d %H%M%S')
)
tweets.df %>%
mutate(
created = created %>%
# Remove zeros.
str_remove_all(pattern = '\\+0000') %>%
# Parse date.
parse_date_time(orders = '%y-%m-%d %H%M%S')
)
tweets.df %<>%
mutate(
created = created %>%
# Remove zeros.
str_remove_all(pattern = '\\+0000') %>%
# Parse date.
parse_date_time(orders = '%y-%m-%d %H%M%S')
)
tweets.df
tweets.df %<>%
mutate(Created_At_Round = created%>% round(units = 'hours') %>% as.POSIXct())
tweets.df
tweets.df %>%
mutate(
created = created %>%
# Remove zeros.
str_remove_all(pattern = '\\+0000') %>%
# Parse date.
parse_date_time(orders = '%y-%m-%d %H%M%S')
)
positive = scan('positive-words.txt', what = 'character', comment.char = ';')
positive = scan('positive-words.txt', what = 'character', comment.char = ';')
negative = scan('negative-words.txt', what = 'character', comment.char = ';')
# add your list of words below as you wish if missing in above read lists
pos.words = c(positive,'upgrade','Congrats','prizes','prize','thanks','thnx',
'Grt','gr8','plz','trending','recovering','brainstorm','leader')
neg.words = c(negative,'wtf','wait','waiting','epicfail','Fight','fighting',
'arrest','no','not')
score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
{
require(plyr)
require(stringr)
# we are giving vector of sentences as input.
# plyr will handle a list or a vector as an "l" for us
# we want a simple array of scores back, so we use "l" + "a" + "ply" = laply:
scores = laply(sentences, function(sentence, pos.words, neg.words) {
# clean up sentences with R's regex-driven global substitute, gsub() function:
sentence = gsub('https://','',sentence)
sentence = gsub('http://','',sentence)
sentence = gsub('[^[:graph:]]', ' ',sentence)
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\\d+', '', sentence)
sentence = str_replace_all(sentence,"[^[:graph:]]", " ")
# and convert to lower case:
sentence = tolower(sentence)
# split into words. str_split is in the stringr package
word.list = str_split(sentence, '\\s+')
# sometimes a list() is one level of hierarchy too much
words = unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
# match() returns the position of the matched term or NA
# we just want a TRUE/FALSE:
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
# TRUE/FALSE will be treated as 1/0 by sum():
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress )
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
analysis <- score.sentiment(cleanText, pos.words, neg.words)
# sentiment score frequency table
table(analysis$score)
analysis %>%
ggplot(aes(x=score)) +
geom_histogram(binwidth = 1, fill = "lightblue")+
ylab("Frequency") +
xlab("sentiment score") +
ggtitle("Distribution of Sentiment scores of the tweets") +
ggeasy::easy_center_title()
neutral <- length(which(analysis$score == 0))
positive <- length(which(analysis$score > 0))
negative <- length(which(analysis$score < 0))
Sentiment <- c("Positive","Neutral","Negative")
Count <- c(positive,neutral,negative)
output <- data.frame(Sentiment,Count)
output$Sentiment<-factor(output$Sentiment,levels=Sentiment)
ggplot(output, aes(x=Sentiment,y=Count))+
geom_bar(stat = "identity", aes(fill = Sentiment))+
ggtitle("Barplot of Sentiment type of 4000 tweets")
text_corpus <- Corpus(VectorSource(cleanText))
text_corpus <- tm_map(text_corpus, content_transformer(tolower))
text_corpus <- tm_map(text_corpus, function(x)removeWords(x,stopwords("english")))
text_corpus <- tm_map(text_corpus, removeWords, c("brexit"))
tdm <- TermDocumentMatrix(text_corpus)
tdm <- as.matrix(tdm)
tdm <- sort(rowSums(tdm), decreasing = TRUE)
tdm <- data.frame(word = names(tdm), freq = tdm)
set.seed(123)
wordcloud(text_corpus, min.freq = 1, max.words = 100, scale = c(2.2,1),
colors=brewer.pal(8, "Dark2"), random.color = T, random.order = F)
wordcloud(text_corpus, min.freq = 1, max.words = 25, scale = c(2.2,1),
colors=brewer.pal(8, "Dark2"), random.color = T, random.order = F)
set.seed(123)
wordcloud(text_corpus, min.freq = 1, max.words = 34, scale = c(2.2,1),
colors=brewer.pal(8, "Dark2"), random.color = T, random.order = F)
ggplot(tdm[1:20,], aes(x=reorder(word, freq), y=freq)) +
geom_bar(stat="identity") +
xlab("Terms") +
ylab("Count") +
coord_flip() +
theme(axis.text=element_text(size=7)) +
ggtitle('Most common word frequency plot') +
ggeasy::easy_center_title()
gc()
gc()
